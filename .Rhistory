x = data_types$col)] <- 3
data_types$dim[grepl(pattern = "smoking",
x = data_types$col)] <- 4
data_types$nclass <- ifelse(data_types$dim > 1, data_types$dim, NA_integer_)
write.csv(data_types, "vambn-extensions-evaluations/HI-VAE/train-data_types.csv", row.names = F, na = "", quote = F)
# -------------------------------------------------
# 7) Create results.csv for HIVAE (hyperparameters)
# -------------------------------------------------
results <- unlist(names(data_all))
results <- results[1:(length(results)-1)]
results <- data.frame("lrates" = rep(0.01, length(results)),
"nbatch" = rep(64, length(results)),
"wdecay" = rep(0, length(results)),
"ydims" = rep(1, length(results)),
"sdims" = rep(1, length(results)),
"files" = paste0(results,".csv"))
results$sdims[grepl(pattern = "lung", results$files)] <- 2
write.csv(results, "vambn-extensions-evaluations/HI-VAE/train-results.csv", row.names = F, na = "", quote = F)
############# README
# For HI-VAE there is no actual imputation, just formatting and saveout of auxiliary variables and HIVAE input data.
# Before this, run the R files clean_format.R->impute_aux.R (scripts with fixed settings) to get the data into the right format.
# After this, run the autoencoder jupyter notebook for HIVAE training
##############
rootpath <- getwd()
newpath <- 'vambn-extensions-evaluations/'
setwd(newpath)
source('helper/make_dummy.R') # create dummies for categorical variables
source('helper/clean_help.R') # check for constant variables
source('helper/fill_na.R') # fill na with mean or most frequent cat
data_out<-'data/data_out/'
data_out_py<-'HI-VAE/data_python/' #originally data/HI-VAE/data_python/
###################### Imputation & AUX
data_all<-readRDS(file = paste0("data/train_data_condensed.rds"))
data_aux=list()
for (datan in names(data_all)){ # for every variable group
# load data & remove SUBJID
data<-data_all[[datan]]
pt<-data$SUBJID
data$SUBJID<-NULL
#if (!grepl('stalone_VIS6|stalone_VIS12|stalone_VIS24|snp_VIS1', datan)){ #JS: does this skip all vargroups with only 1 column, as they can't be accessed by [x,y]?
if (!grepl('stalone_VIS00', datan)){
# remove bad data
data=data[,includeVar(data), drop = FALSE]
#data=data[,rmMiss(data)] #JS: this would remove all data from later visits, as many participants are too young
}
###################### AUX variables
# make AUX columns and save in separate list (with SUBJID)
# AUX if A) stalone missing, or B) all/any entries of a vargroup are missing: which one?
nms<-colnames(data)
if (grepl('stalone', datan)){
dataux<-as.data.frame(sapply(as.data.frame(is.na(data)), as.numeric)) #A
dataux<-as.data.frame(sapply(dataux,factor))
colnames(dataux)<-paste('AUX',nms,sep='_')
}else{
dataux<-data.frame(factor(apply(data,1,function(x) as.numeric(any(is.na(x)))))) #B: all( or any(is.na(x)) ?
colnames(dataux)<-paste('AUX',datan,sep='_')
}
# update AUX list
dataux$SUBJID<-pt
data_aux[[datan]]<-dataux
###################### Imputation
print(datan)
if (grepl('stalone', datan)){
data<-fillna(data) # if standalone data, mean and most frequent class imputation
data$SA_deceased_VIS00 <- as.factor(data$SA_deceased_VIS00)
data$SA_race_VIS00 <- as.factor(data$SA_race_VIS00)
data$SA_gender_VIS00 <- as.factor(data$SA_gender_VIS00)
data$SA_signed_icf_VIS00 <- as.factor(data$SA_signed_icf_VIS00)
}
# if (!grepl('stalone_VIS6|stalone_VIS12|stalone_VIS24|snp_VIS1', datan)){
if (!grepl('stalone_VIS00', datan)){
# remove bad data
data=data[,includeVar(data),drop = FALSE]
#data=data[,rmMiss(data)] #JS: this would remove all data from later visits, as many participants are too young
}
# add ppt variable and update data list
data$SUBJID <- pt
data_all[[datan]]<-data
# save out csv's of scaled continous and dummy coded categorical data for autoencoders
pt<-data$SUBJID
data$SUBJID<-NULL
#missing write
if (!grepl('stalone', datan))
write.table(which(is.na(data), arr.ind=TRUE),paste0(data_out_py,datan,'_missing.csv'),sep=',',row.names = F,col.names = F,quote=F)
#data write
if (!grepl('stalone', datan))
write.table(data,paste0(data_out_py,datan,'.csv'),sep=',',row.names = F,col.names = F,quote=F, na = "NaN")
write.table(as.character(pt),paste0('HI-VAE/python_names/',datan,'_subj.csv'),sep=',',row.names = F,col.names = T,quote=T, na = "NaN") #originally data/HI-VAE/python_names/
write.table(colnames(data),paste0('HI-VAE/python_names/',datan,'_cols.csv'),sep=',',row.names = F,col.names = T,quote=T, na = "NaN") #originally data/HI-VAE/python_names/
}
# save all
saveRDS(data_all, file = paste0(data_out,'train_data_all_imp.rds'))
saveRDS(data_aux, file = paste0(data_out,'train_data_aux.rds'))
setwd(rootpath)
train_wide[, lapply(.SD, function(x) all(is.na(x))), ]
train_wide[, lapply(.SD, function(x) all(is.na(x)))]
train_wide <- data.table(train_wide)
train_wide[, lapply(.SD, function(x) all(is.na(x)))]
train_wide[, lapply(.SD, function(x) mean(is.na(x)))]
View(train_wide)
miss <- train_wide[, lapply(.SD, function(x) mean(is.na(x)))]
View(miss)
miss <- train_wide[, lapply(.SD, function(x) sum(!is.na(x)))]
mean(1)
######### return column i's where variance is not 0
includeVar <- function(dat) {
out <- lapply(dat, function(x) length(unique(x[!is.na(x)])))
want <- which(out > 1)
unlist(want)
}
View(minority_vambn)
View(df)
setwd("C:/Users/Katariina Perkonoja/OneDrive - O365 Turun yliopisto/phd/osatyot/method_paper/src")
library(data.table)
library(arrow)
library(tidyverse) # covers ggplot2, dplyr, tidyr, readr, etc.
library(Hmisc)
library(missForest)
library(arules)
library(mclust)
library(rpart)
library(bnlearn)
library(parallel)
library(randomForest)
library(randomForestSRC)
library(synthdata) # TODO check overlapping packages
source("data_preproc.R")
source("data_preproc.R")
source("impute_aux_first8.R")
source("data_preproc.R")
source("impute_aux_first8.R")
# Now run HI-VAE/1_first8_HIVAE_training.py
# Then run bnet_first8.R **until row 72**
# Then run helper/make_bl_wl_first8.py
# Then run the rest of bnet_first8.R
# Then run HI-VAE/2_first8_HIVAE_VP_decoding_and_counterfactuals.py
# Then run HI-VAE/first8_writer.py
source("run_bangltm.R") # TODO
source("data_postproc.R")
getwd()
setwd("C:/Users/Katariina/OneDrive - O365 Turun yliopisto/phd/osatyot/method_paper/results/simulations")
readRDS("simulations_v10.rd")
################################################################################
######## SDG Benchmarking Pipeline for Longitudinal Data with EXP Error ########
################################################################################
library(data.table)
library(nlme)
library(MASS)
library(ggplot2)
library(parallel)
library(synthdata)   # for bangltm()
library(data.table)
library(nlme)
library(MASS)
library(ggplot2)
library(parallel)
set.seed(123)
###############################################################
### PARAMETERS
###############################################################
###############################################################
### PARAMETERS
###############################################################
n_subj <- 500
n_time <- 6
beta0 <- 30      # baseline level (arbitrary)
## Fixed effects
beta1 <- 0.2
beta2 <- 10      # male vs female difference
beta3 <- 1.2      # per category step
beta4 <- 0.20     # control slope
beta5 <- 0      # intervention starts 0 units higher at baseline
beta6 <- -0.40    # extra slope for intervention
## Random part / correlation
tau   <- 0.05      # random intercept SD (still sizeable, but not huge)
sigma <- 0.7        # total within-subject SD
phi   <- 0.15     # exponential correlation range
rho   <- 0.1     # 10% nugget, 90% correlated
true_params <- list(
beta0 = beta0,
beta1 = beta1,
beta2 = beta2,
beta3 = beta3,
beta4 = beta4,
beta5 = beta5,
beta6 = beta6,
tau   = tau,
sigma = sigma,
phi   = phi,
rho   = rho
)
simulate_dataset_exp <- function() {
id        <- rep(1:n_subj, each = n_time)
timepoint <- rep(0:(n_time - 1), times = n_subj)
group    <- rep(sample(0:1, n_subj, replace = TRUE), each = n_time)
age      <- rep(rnorm(n_subj, 45, 10), each = n_time)
sex      <- rep(sample(0:1, n_subj, replace = TRUE), each = n_time)
category <- rep(sample(1:3, n_subj, replace = TRUE, prob = c(.05, .25, .7)),
each = n_time)
## Random intercepts
b0 <- rep(rnorm(n_subj, 0, tau), each = n_time)
## Correlated residual part (EXP) + measurement error (nugget)
times <- 0:(n_time - 1)
# variance of correlated process and nugget part
sigma_cor <- sigma * sqrt(1 - rho)
sigma_me  <- sigma * sqrt(rho)
Cov_cor <- outer(
times, times,
function(t1, t2) sigma_cor^2 * exp(-phi * abs(t1 - t2))
)
# correlated EXP part
W_cor <- unlist(lapply(
1:n_subj,
function(i) as.numeric(t(chol(Cov_cor)) %*% rnorm(n_time))
))
# independent measurement error (nugget)
eps_me <- rnorm(n_subj * n_time, mean = 0, sd = sigma_me)
mu <- beta0 +
beta1 * age +
beta2 * sex +
beta3 * category +
beta4 * timepoint +
beta5 * group +
beta6 * (group * timepoint)
# response = mean + random intercept + correlated noise + measurement error
y <- mu + b0 + W_cor + eps_me
data.table(
id        = id,
timepoint = timepoint,
sex       = factor(sex, labels = c("female", "male")),
age       = age,
group     = factor(group, labels = c("control", "intervention")),
category  = category,
response  = y
)
}
#### 1. SIMULATE DATA
sim <- simulate_dataset_exp()
View(sim)
write.csv(sim, = "demo.csv", row.names = F)
write.csv(sim, "demo.csv", row.names = F)
library(readr)
demo <- read_csv("demo.csv")
View(demo)
setwd("C:/Users/Katariina/Desktop/Temporal-preservation-metrics")
library(readr)
demo <- read_csv("data/demo.csv")
View(demo)
length(unique(demo$id))
# Import demo data and split it into two
# (the other part is treated as synthetic data)
demo <- read_csv("data/demo.csv")
sampled_ids <- sample(unique(demo$id), size = length(unique(demo$id))/2))
sampled_ids <- sample(unique(demo$id), size = length(unique(demo$id))/2)
# keep all rows for those subjects
demo1 <- demo[demo$id %in% sampled_ids, ]
demo2 <- demo[!(demo$id %in% sampled_ids), ]
View(demo1)
View(demo2)
View(demo)
View(demo)
analyze_and_plot_data(
original = demo1,
synthetic = demo2,
id_col = "id",
time_col = "timepoint",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
source("src/helpers.R")
source("src/analyze_and_plot.R")
analyze_and_plot_data(
original = demo1,
synthetic = demo2,
id_col = "id",
time_col = "timepoint",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
# Load necessary libraries
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(stringr)
library(gtable)
library(grid)
library(patchwork)
library(scales)
library(reshape2)
library(data.table)
library(RcppHungarian)
analyze_and_plot_data(
original = demo1,
synthetic = demo2,
id_col = "id",
time_col = "timepoint",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
# Load necessary libraries
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
library(stringr)
library(gtable)
library(grid)
library(patchwork)
library(scales)
library(reshape2)
library(data.table)
library(RcppHungarian)
source("src/helpers.R")
source("src/analyze_and_plot.R")
analyze_and_plot_data(
original = demo1,
synthetic = demo2,
id_col = "id",
time_col = "timepoint",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
analyze_and_plot_data(
original = demo1[, c("id", "timepoint", "response")],
synthetic = demo2[, c("id", "timepoint", "response")],
id_col = "id",
time_col = "timepoint",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
colnames(demo)
colnames(demo)[1] <- "Subject ID"
colnames(demo)[2] <- "Time"
colnames(demo)[3:7] <- str_to_sentence(colnames(demo)[3:7])
colnames(demo)
write.csv(demo, "data/demo.csv", row.names = F)
# Import demo data and split it into two
# (the other part is treated as synthetic data)
demo <- read_csv("data/demo.csv")
View(demo)
sampled_ids <- sample(unique(demo$`Subject ID`), size = length(unique(demo$`Subject ID`))/2)
# keep all rows for those subjects
demo1 <- demo[demo$`Subject ID` %in% sampled_ids, ]
demo2 <- demo[!(demo$`Subject ID` %in% sampled_ids), ]
analyze_and_plot_data(
original = demo1[, c("Subject ID", "Time", "Response")],
synthetic = demo2[, c("Subject ID", "Time", "Response")],
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
# Documentation in the respective .R file
# In the current implementation, the id column is expected to be "Subject ID"
analyze_and_plot_data(
original = demo1,
synthetic = demo2,
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
# Documentation in the respective .R file
# In the current implementation, the id column is expected to be "Subject ID"
# and the analysis work only for id, time and time varying variables
analyze_and_plot_data(
original = demo1[, c("Subject ID", "Time", "Response")],
synthetic = demo2[, c("Subject ID", "Time", "Response")],
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
source("src/analyze_and_plot.R")
# Documentation in the respective .R file
# In the current implementation, the id column is expected to be "Subject ID"
# and the analysis work only for id, time and time varying variables
analyze_and_plot_data(
original = demo1[, c("Subject ID", "Time", "Response")],
synthetic = demo2[, c("Subject ID", "Time", "Response")],
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
source("src/analyze_and_plot.R")
# Documentation in the respective .R file
# In the current implementation, the id column is expected to be "Subject ID"
# and the analysis work only for id, time and time varying variables
analyze_and_plot_data(
original = demo1[, c("Subject ID", "Time", "Response")],
synthetic = demo2[, c("Subject ID", "Time", "Response")],
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
source("src/analyze_and_plot.R")
source("src/analyze_and_plot.R")
# Documentation in the respective .R file
# In the current implementation, the id column is expected to be "Subject ID"
# and the analysis work only for id, time and time varying variables
analyze_and_plot_data(
original = demo1[, c("Subject ID", "Time", "Response")],
synthetic = demo2[, c("Subject ID", "Time", "Response")],
id_col = "Subject ID",
time_col = "Time",
max_time = 5,
x_interval = 1,
bandwidth = 2,
quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95),
save_path = NULL,  # Set to NULL to print to screen,
mean_quantile = TRUE,
cat_prop = TRUE,
rank_stab = TRUE,
meas_sim = TRUE,
meas_n = 150,
meas_iter = 100
)
